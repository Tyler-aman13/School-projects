---
title: "Paper 1 Appendix"
author: "Tyler Aman"
date: "1/18/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### R Block

```{r R Block}
#libraries
library(gridExtra)
com_zones = read.csv("http://dept.stat.lsa.umich.edu/~bbh/s485/data/mobility1.csv")
attach(com_zones)
rownames(com_zones) = com_zones[,1]
com_zones = com_zones[,-1]
com_zones[,'X'] = n.lowstart*p.upmover
#subsetting data 
com_zones_MW = com_zones[region == levels(region)[1],]
com_zones_NE = com_zones[region == levels(region)[2],]
com_zones_S = com_zones[region == levels(region)[3],]
com_zones_W = com_zones[region == levels(region)[4],]
#setting up dataset for CIs
com_zones[,'lower'] = rep(NA, length(n.lowstart))
com_zones[,'upper'] = rep(NA, length(n.lowstart))
head(com_zones)

```

### Overview

####Interval Usage For Part A

For This Paper the researcher used the Clopper - Pearson Confidence Interval(CI) for estimating the 95% CI for $\hat{p}$ which is a direct inversion of the equal-tailed binomial test as apposed to he normal  approximation of this test.(Brown, Cai, DasGupta, 2001). This interval is mathematically defined as (assuming $X = x$ is observed):
\[
Pr_p(X \geq x) = \alpha/2 \\ and \\ Pr_p(X \leq x) = \alpha/2
\]
where each is the lower and upper tail of 100(1 - $\alpha$)% CI. This according to Brown, Cai, and DasGupta can mathematically be shown to be the $\alpha/2$ quantile of the Beta($x, n - x + 1$) distribution for the lower bound and the $1 - \alpha/2$ quantile of the Beta($x + 1, n - x$) distribution for the upper bound (2001). The reasoning behind using this CI is due to its simplicity in producing its boundaries and not being limited by $n$ being used. Issues with this interval involve its coverage being guaranteed to be over the 95% mark although this research values the conservative nature of this interval along with its ease of calcuation and none limits involving $n$.


#### Test used in Part B 

The test being used in part B will be the Likelihood Ratio Test(LRT) which will be used to help test two seperate hypothesis as follows: <br />
<br />
first: $H_{o1}: p_{region} = p_{o1}$ <br />
<br />
second: $H_{o2}: p_{USA} = p_{o2}$ <br />
<br />
Breaking these to hypotheses down the first is stating that the null hypothesis is that each region (Midwest, South, etc.) has a common proportion for economic mobility. If this is found to be true for the majority of the individual regions a second test will be created to test whether there is a common proportion accross the United States as a whole, or the regions that support the first null hypothesis. The LRT is defined under the null hypothesis: 
\[
2[log(\prod_{i = 1}^{J}{L_i(\hat p_i)}) - log(\prod_{i = 1}^{J}{L_i(\hat p)})] \sim \chi^2_{df(H_a)-df(H_o)}
\]
Here $J$ is defined as the number of binomial distribution observation there are and $L(\hat p)$ is the likelihood function of a binomial distribution given $n$ and $x$ where $L(p) =$  ${{n} \choose {x}} p^x(1 - p)^{n-x}$ and violating the null hypothesis at .05 level means the $LRT_{stat} \geq \chi^2_{df(H_a)-df(H_o)}(.95)$ which is the .95 quantile of that specific $\chi^2$ distribution. $\hat p$ in these functions is defined $\hat p_{mle} = max L(p)$ also known as the maximum likelihood estimator of this binomial distribution which for one instance of $L(p)$ can be shown to be $\hat p_{mle_i} = X_i/n_i$ or the number of observed success over number of observations already in our dataset as p.upmover. for both tests assuming a common proportion, $\hat p_{mle}$ is defined $\hat p_{mle} = \sum_{i =1}^J X_i / \sum_{i =1}^J n_i$ or the joint success over all binomial distributions as defined from 1 to $J$ over all observations over that region.

#### Overview of Coding Scheme

coding will follow a pattern of first calculating the CI's of each zone using the Clopper-Pearson CI recommendations and R's qbeta function to find the appropriate number. Next, place the boundaries into their appropriate colmumn and row of the com_zones data. I will also out put a list of the CI's for each zone. After, I will run the LRT as defined above using the log version of the dbinom function in R. and report the Likeihood Ratio Test Confidence Intervals for those proportions which showcase support for the null hypothesis of common proportions either across regions or the united states as a whole




```{r Code}
#code for part a
com_zones[,'lower'] = round(qbeta(.025, com_zones[,'X'], n.lowstart - com_zones[,'X'] + 1), 3)
com_zones[,'upper'] = round(qbeta(.975, com_zones[,'X'] + 1, n.lowstart - com_zones[,'X']),3)

g1 = tableGrob(round(com_zones[1:8,c(8,6,9)],3))
g2 = tableGrob(round(com_zones[9:16,c(8,6,9)],3))
g3 = tableGrob(round(com_zones[17:24,c(8,6,9)],3))
g4 = tableGrob(round(com_zones[25:32,c(8,6,9)],3))
g5 = tableGrob(round(com_zones[33:40,c(8,6,9)],3))

grid.arrange(gtable_combine(g1, g2, along = 2),gtable_combine(g3, g4, along = 2), ncol = 2)
grid.arrange(g5)
#code for part b 
#function giving the log Likelihood
#due to being used with more than one context it is more flexible than other function
Log_like = function(obs, suc, prob.1){
  sum(dbinom(suc, obs, p = prob.1, log = T))
}
#common proportions defined by each subset
prob.MW = sum(com_zones_MW$X)/sum(com_zones_MW$n.lowstart)
prob.NE = sum(com_zones_NE$X)/sum(com_zones_NE$n.lowstart)
prob.S = sum(com_zones_S$X)/sum(com_zones_S$n.lowstart)
prob.W = sum(com_zones_W$X)/sum(com_zones_W$n.lowstart)
prob.USA = sum(com_zones_MW$X + com_zones_NE$X + com_zones_W$X)/sum(com_zones_MW$n.lowstart + com_zones_NE$n.lowstart + com_zones_W$n.lowstart)
#LRT_stat for each region
LRT_Stat = rep(NA,5)
#Midwest
LRT_Stat[1] = 2*(Log_like(com_zones_MW$n.lowstart,com_zones_MW$X, com_zones_MW$p.upmover) - Log_like(com_zones_MW$n.lowstart,com_zones_MW$X, prob.MW))

#North East
LRT_Stat[2] = 2*(Log_like(com_zones_NE$n.lowstart,com_zones_NE$X, com_zones_NE$p.upmover) - Log_like(com_zones_NE$n.lowstart,com_zones_NE$X, prob.NE))

#South
LRT_Stat[3] = 2*(Log_like(com_zones_S$n.lowstart,com_zones_S$X, com_zones_S$p.upmover) - Log_like(com_zones_S$n.lowstart,com_zones_S$X, prob.S))

#West
LRT_Stat[4] = 2*(Log_like(com_zones_W$n.lowstart,com_zones_W$X, com_zones_W$p.upmover) - Log_like(com_zones_W$n.lowstart,com_zones_W$X, prob.W))
#all tests at once
p_values = rep(NA, 5)
df = c(9,9,9,9,2)
LRT_Stat
qchisq(.95, 9, lower = T)
p_values[1:4] = pchisq(LRT_Stat, 9, lower = F)

#testing for overall proportion
prob.all = c(prob.MW,prob.NE,prob.S,prob.W, prob.USA)
X.region = c(sum(com_zones_MW$X), sum(com_zones_NE$X), sum(com_zones_S$X), sum(com_zones_W$X))
n.region = c(sum(com_zones_MW$n.lowstart),sum(com_zones_NE$n.lowstart), sum(com_zones_S$n.lowstart), sum(com_zones_W$n.lowstart))
LRT_Stat[5] = 2*(Log_like(n.region[c(1,2,4)],X.region[c(1,2,4)],prob.all[c(1,2,4)]) - Log_like(n.region[c(1,2,4)],X.region[c(1,2,4)], prob.USA)) 
p_values[5] = pchisq(LRT_Stat[5], 2, lower = F)
qchisq(.95, 2)

#functions for getting roots for the individual sections confidence intervals
getroot.MW = function(p){
  Log_like(com_zones_MW$n.lowstart, com_zones_MW$X, com_zones_MW$p.upmover) - Log_like(com_zones_MW$n.lowstart, com_zones_MW$X, p) - .5*qchisq(.95,9)
}

getroot.NE = function(p){
  Log_like(com_zones_NE$n.lowstart, com_zones_NE$X, com_zones_NE$p.upmover) - Log_like(com_zones_NE$n.lowstart, com_zones_NE$X, p) - .5*qchisq(.95,9)
}

getroot.W = function(p){
  Log_like(com_zones_W$n.lowstart, com_zones_W$X, com_zones_W$p.upmover) - Log_like(com_zones_W$n.lowstart, com_zones_W$X, p) - .5*qchisq(.95,9)
}

lw_bound = rep(NA,5)
up_bound = rep(NA,5)

#midwest
lw_bound[1] = uniroot(getroot.MW, c(0,prob.all[1]))[[1]]
up_bound[1] = uniroot(getroot.MW, c(prob.all[1],1))[[1]]

#NE
lw_bound[2] = uniroot(getroot.NE, c(0,prob.all[2]))[[1]]
up_bound[2] = uniroot(getroot.NE, c(prob.all[2],1))[[1]]

#west
lw_bound[4] = uniroot(getroot.W, c(0,prob.all[4]))[[1]]
up_bound[4] = uniroot(getroot.W, c(prob.all[4],1))[[1]]

#results 
LRT_Results = round(data.frame(LRT_Stat,p_values,lw_bound,prob.all,up_bound,df, row.names = c('Midwest','Northeast','South','West','Overall')),3)
grid.arrange(tableGrob(LRT_Results))

```

